{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea18c380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\moina\\\\Downloads\\\\chatapp\\\\backend\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a906b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b7ad50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\moina\\\\Downloads\\\\med-chatbot'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0100e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67c5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed234d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ba39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52e43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f1f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 5859\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cb89808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "786a1c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moina\\AppData\\Local\\Temp\\ipykernel_24860\\220312833.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0eae4657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0610ca77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "692f011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "NVIDIA_API_KEY=os.environ.get('NVIDIA_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"NVIDIA_API_KEY\"] = NVIDIA_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63254071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12278ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# Get API key from environment\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "index_name = \"medical-bot\"  # change if desired\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80b23846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87c5172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71ddc619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x1e302b59d20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d41415eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88aeb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moina\\AppData\\Local\\Temp\\ipykernel_24860\\2768571858.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrived_docs = retriver.get_relevant_documents(\"What is the Acne\")\n"
     ]
    }
   ],
   "source": [
    "retrived_docs = retriver.get_relevant_documents(\"What is the Acne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "238853fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='807288c9-37ea-400a-8316-d52bc03c5213', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='26c00228-2648-4e33-a4c2-e15a102e7521', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='5943dd2f-8cf4-4f8a-8501-8a34f695039e', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrived_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "066ddcea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.openai'"
     ]
    }
   ],
   "source": [
    "from langchain.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.4, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447675fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e28d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415afe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ad57d7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 78\u001b[0m\n\u001b[0;32m     70\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m     71\u001b[0m     [\n\u001b[0;32m     72\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system_prompt),\n\u001b[0;32m     73\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     74\u001b[0m     ]\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     77\u001b[0m question_answer_chain \u001b[38;5;241m=\u001b[39m create_stuff_documents_chain(chatModel, prompt)\n\u001b[1;32m---> 78\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m create_retrieval_chain(\u001b[43mretriever\u001b[49m, question_answer_chain)\n\u001b[0;32m     80\u001b[0m response \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is Acromegaly and gigantism?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Any, List, Optional\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.outputs import LLMResult\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. Create NVIDIA client\n",
    "nvidia_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Wrap NVIDIA into a LangChain-compatible LLM\n",
    "class NvidiaLLM(LLM):\n",
    "    model: str = \"openai/gpt-oss-120b\"\n",
    "    temperature: float = 0.4\n",
    "    max_tokens: int = 500\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nvidia-llm\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        completion = nvidia_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "        )\n",
    "        return completion.choices[0].message[\"content\"]\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        responses = [self._call(p, stop=stop, run_manager=run_manager, **kwargs) for p in prompts]\n",
    "        return LLMResult(generations=[[{\"text\": r}] for r in responses])\n",
    "\n",
    "\n",
    "# 3. Use it instead of OpenAI\n",
    "chatModel = NvidiaLLM()\n",
    "\n",
    "# 4. Keep the rest of your RAG chain the same\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# --- PDF Loading ---\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    return loader.load()\n",
    "\n",
    "extracted_data = load_pdf_file(data='Data/')\n",
    "\n",
    "# --- Text Splitting ---\n",
    "def text_split(extracted_data):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    return splitter.split_documents(extracted_data)\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "\n",
    "# --- Embeddings ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Pinecone Setup ---\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"medical-bot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# --- NVIDIA LLM wrapper ---\n",
    "from typing import Any, List, Optional\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.outputs import LLMResult\n",
    "from openai import OpenAI\n",
    "\n",
    "nvidia_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "class NvidiaLLM(LLM):\n",
    "    model: str = \"openai/gpt-oss-120b\"\n",
    "    temperature: float = 0.4\n",
    "    max_tokens: int = 500\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nvidia-llm\"\n",
    "\n",
    "    def _call(\n",
    "        self, prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        completion = nvidia_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "        )\n",
    "        return completion.choices[0].message[\"content\"]\n",
    "\n",
    "    def _generate(\n",
    "        self, prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        responses = [self._call(p, stop=stop, run_manager=run_manager, **kwargs) for p in prompts]\n",
    "        return LLMResult(generations=[[{\"text\": r}] for r in responses])\n",
    "\n",
    "chatModel = NvidiaLLM()\n",
    "\n",
    "# --- Prompt & RAG Chain ---\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# --- Test Query ---\n",
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello from NVIDIA\"}],\n",
    "    max_tokens=50,\n",
    ")\n",
    "print(resp.choices[0].message[\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73089aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"NVIDIA_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ce62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.getenv('NVIDIA_API_KEY')}\"}\n",
    "url = \"https://integrate.api.nvidia.com/v1/status\"  # or their recommended test endpoint\n",
    "r = requests.get(url, headers=headers)\n",
    "print(r.status_code, r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb13749",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv langchain langchain-nvidia-ai-endpoints pypdf sentence-transformers pinecone-client langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moina\\AppData\\Local\\Temp\\ipykernel_24860\\3455666486.py:66: DeprecationWarning: The 'max_tokens' parameter is deprecated and will be removed in a future version. Please use 'max_completion_tokens' instead.\n",
      "  nvidia_llm = ChatNVIDIA(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Load environment variables\n",
    "# ----------------------------\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads NVIDIA_API_KEY and PINECONE_API_KEY from .env\n",
    "\n",
    "NVIDIA_API_KEY = os.getenv(\"NVIDIA_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# ----------------------------\n",
    "# PDF Loading\n",
    "# ----------------------------\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "def load_pdf_files(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    return loader.load()\n",
    "\n",
    "documents = load_pdf_files(\"Data/\")  # replace with your folder\n",
    "\n",
    "# ----------------------------\n",
    "# Text Splitting\n",
    "# ----------------------------\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "text_chunks = split_text(documents)\n",
    "\n",
    "# ----------------------------\n",
    "# Embeddings\n",
    "# ----------------------------\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ----------------------------\n",
    "# Pinecone Setup\n",
    "# ----------------------------\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"medical-bot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# ----------------------------\n",
    "# NVIDIA LLM Setup\n",
    "# ----------------------------\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "nvidia_llm = ChatNVIDIA(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    api_key=NVIDIA_API_KEY,\n",
    "    temperature=0.4,\n",
    "    top_p=1,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Prompt Template\n",
    "# ----------------------------\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, say so. \"\n",
    "    \"Keep the answer concise, max three sentences.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(nvidia_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# ----------------------------\n",
    "# Test Query\n",
    "# ----------------------------\n",
    "response = rag_chain.invoke({\"input\": \"What is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(\"PINECONE_API_KEY:\", os.getenv(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from parent directory\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"PINECONE_API_KEY loaded:\", bool(os.getenv(\"PINECONE_API_KEY\")))\n",
    "\n",
    "try:\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    \n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    \n",
    "    # List existing indexes first\n",
    "    existing_indexes = pc.list_indexes()\n",
    "    print(\"Existing indexes:\", existing_indexes)\n",
    "    \n",
    "    index_name = \"medical-bot\"\n",
    "    \n",
    "    if index_name not in [idx.name for idx in existing_indexes]:\n",
    "        print(f\"Creating new index: {index_name}\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=384,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Index {index_name} already exists\")\n",
    "    \n",
    "    # Test connection to the index\n",
    "    index = pc.Index(index_name)\n",
    "    stats = index.describe_index_stats()\n",
    "    print(\"Index stats:\", stats)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Pinecone error:\", str(e))\n",
    "    print(\"Error type:\", type(e).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce395e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | findstr pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "  # must \n",
    "  # run first\n",
    "\n",
    "load_dotenv()  # must run first\n",
    "\n",
    "print(os.getenv(\"NVIDIA_API_KEY\"))  # should show your key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c068187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb859726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what pinecone packages are installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"list\"], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    \n",
    "    # Filter for pinecone packages\n",
    "    pinecone_packages = [line for line in result.stdout.split('\\n') \n",
    "                        if 'pinecone' in line.lower()]\n",
    "    \n",
    "    if pinecone_packages:\n",
    "        print(\"Pinecone packages found:\")\n",
    "        for pkg in pinecone_packages:\n",
    "            print(f\"  {pkg}\")\n",
    "    else:\n",
    "        print(\"No pinecone packages found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking packages: {e}\")\n",
    "\n",
    "# Also try to import and check what we have\n",
    "try:\n",
    "    import pinecone\n",
    "    print(f\"\\nPinecone module location: {pinecone.__file__}\")\n",
    "    \n",
    "    # Check available attributes\n",
    "    print(\"Available attributes:\", [attr for attr in dir(pinecone) if not attr.startswith('_')])\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Cannot import pinecone: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Other error with pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Pinecone version - uninstall old version and install new one\n",
    "!pip uninstall pinecone pinecone-plugin-assistant pinecone-plugin-interface -y\n",
    "!pip install \"pinecone-client>=3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new Pinecone installation (run after kernel restart)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables\n",
    "\n",
    "try:\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    print(\"✅ Pinecone imports successful!\")\n",
    "    \n",
    "    # Test connection\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    print(\"✅ Pinecone client initialized!\")\n",
    "    \n",
    "    # List indexes\n",
    "    indexes = pc.list_indexes()\n",
    "    print(\"Available indexes:\", indexes)\n",
    "    \n",
    "    # Check if medical-bot index exists, create if not\n",
    "    index_name = \"medical-bot\"\n",
    "    if not any(idx.name == index_name for idx in indexes):\n",
    "        print(f\"Creating index: {index_name}\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=384,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        )\n",
    "        print(f\"✅ Index {index_name} created!\")\n",
    "    else:\n",
    "        print(f\"✅ Index {index_name} already exists!\")\n",
    "    \n",
    "    # Test index connection\n",
    "    index = pc.Index(index_name)\n",
    "    stats = index.describe_index_stats()\n",
    "    print(\"Index stats:\", stats)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"❌ Import error:\", e)\n",
    "    print(\"Make sure you've restarted the kernel after installing the new version\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ba442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c72cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Pinecone package name - uninstall pinecone-client and install pinecone\n",
    "!pip uninstall pinecone-client -y\n",
    "!pip install pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de34341",
   "metadata": {},
   "source": [
    "# Clean NVIDIA-Only RAG Implementation\n",
    "**Using NVIDIA's free `openai/gpt-oss-120b` model instead of OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 637 documents, split into 5859 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moina\\AppData\\Local\\Temp\\ipykernel_26984\\2234481858.py:35: LangChainDeprecationWarning: The class `HuggingFaceHubEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpointEmbeddings``.\n",
      "  embeddings = HuggingFaceHubEmbeddings(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHubEmbeddings\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'sentence-tra...rgs': {'device': 'cpu'}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf_token:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing Hugging Face API token (HF_API_TOKEN)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceHubEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L12-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"intfloat/e5-small\"\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ensures it runs on CPU\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Documents and embeddings ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\moina\\.conda\\envs\\medibot\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:224\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     emit_warning()\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moina\\.conda\\envs\\medibot\\lib\\site-packages\\pydantic\\main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    260\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHubEmbeddings\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'sentence-tra...rgs': {'device': 'cpu'}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "# COMPLETE NVIDIA-ONLY RAG SETUP (No OpenAI needed)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ================================\n",
    "# 1. Document Loading & Processing\n",
    "# ================================\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def load_and_process_documents():\n",
    "    # Load PDFs\n",
    "    loader = DirectoryLoader(\"Data/\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} documents, split into {len(text_chunks)} chunks\")\n",
    "    return text_chunks\n",
    "\n",
    "# Load documents\n",
    "text_chunks = load_and_process_documents()\n",
    "\n",
    "# Create embeddings (free local model)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"✅ Documents and embeddings ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebba044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moina\\.conda\\envs\\medibot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using existing index: medical-bot\n",
      "✅ Pinecone vector store ready!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2. Pinecone Vector Database Setup\n",
    "# ================================\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"medical-bot\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Matches HuggingFace embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(f\"✅ Created new index: {index_name}\")\n",
    "else:\n",
    "    print(f\"✅ Using existing index: {index_name}\")\n",
    "\n",
    "# Connect to vector store\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name, \n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"✅ Pinecone vector store ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a1b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NVIDIA API working!\n",
      "Response: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# 3. NVIDIA API Setup (FREE!)\n",
    "# ================================\n",
    "from openai import OpenAI\n",
    "\n",
    "# NVIDIA API client using OpenAI-compatible interface\n",
    "nvidia_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "# Test NVIDIA API connection\n",
    "def test_nvidia_api():\n",
    "    try:\n",
    "        response = nvidia_client.chat.completions.create(\n",
    "            model=\"openai/gpt-oss-20b\",  # FREE model\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(\"✅ NVIDIA API working!\")\n",
    "        print(\"Response:\", response.choices[0].message.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"❌ NVIDIA API Error:\", str(e))\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "test_nvidia_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1cf46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NVIDIA LLM wrapper ready!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 4. LangChain Integration with NVIDIA\n",
    "# ================================\n",
    "from typing import Any, List, Optional\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "class NvidiaLLM(LLM):\n",
    "    \"\"\"Custom LangChain LLM wrapper for NVIDIA API\"\"\"\n",
    "    \n",
    "    model: str = \"openai/gpt-oss-20b\"\n",
    "    temperature: float = 0.4\n",
    "    max_tokens: int = 500\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nvidia-llm\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Make API call to NVIDIA\"\"\"\n",
    "        try:\n",
    "            completion = nvidia_client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Generate responses for multiple prompts\"\"\"\n",
    "        responses = [self._call(p, stop=stop, run_manager=run_manager, **kwargs) for p in prompts]\n",
    "        return LLMResult(generations=[[{\"text\": r}] for r in responses])\n",
    "\n",
    "# Create the LLM instance\n",
    "nvidia_llm = NvidiaLLM()\n",
    "\n",
    "print(\"✅ NVIDIA LLM wrapper ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b61192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG chain ready with NVIDIA LLM!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5. RAG Chain Setup\n",
    "# ================================\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# Create medical assistant prompt\n",
    "system_prompt = (\n",
    "    \"You are a knowledgeable medical assistant. \"\n",
    "    \"Use the following medical documentation to answer questions accurately. \"\n",
    "    \"If you don't know the answer based on the provided context, say so clearly. \"\n",
    "    \"Provide concise, helpful medical information. \"\n",
    "    \"Always recommend consulting healthcare professionals for medical advice.\\n\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the RAG chain\n",
    "question_answer_chain = create_stuff_documents_chain(nvidia_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "print(\"✅ RAG chain ready with NVIDIA LLM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2cbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏥 Testing Medical Chatbot with NVIDIA's Free GPT-OSS-20B:\n",
      "============================================================\n",
      "\n",
      "❓ Question 1: how to cure Acne?\n",
      "----------------------------------------\n",
      "🤖 Answer: Error: Expecting value: line 1 column 1 (char 0)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 6. Test the Complete System\n",
    "# ================================\n",
    "\n",
    "def ask_medical_question(question: str) -> str:\n",
    "    \"\"\"Ask a question to the medical chatbot\"\"\"\n",
    "    try:\n",
    "        response = rag_chain.invoke({\"input\": question})\n",
    "        return response[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"how to cure Acne?\"\n",
    "]\n",
    "\n",
    "print(\"🏥 Testing Medical Chatbot with NVIDIA's Free GPT-OSS-20B:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n❓ Question {i}: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    answer = ask_medical_question(question)\n",
    "    print(f\"🤖 Answer: {answer}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f061fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moina\\AppData\\Local\\Temp\\ipykernel_26984\\1336211573.py:43: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acidosis see Respiratory acidosis; Renal\n",
      "tubular acidosis; Metabolic acidosis\n",
      "Acne\n",
      "Definition\n",
      "Acne is a common skin disease characterized by\n",
      "pimples on the face, chest, and back. It occurs when the\n",
      "pores of the skin become clogged with oil, dead skin\n",
      "cells, and bacteria.\n",
      "Description\n",
      "Acne vulgaris, t ...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Load & split documents\n",
    "# -------------------------------\n",
    "loader = DirectoryLoader(\"Data/\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. HF embeddings\n",
    "# -------------------------------\n",
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    repo_id=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    huggingfacehub_api_token=HF_API_TOKEN\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Pinecone\n",
    "# -------------------------------\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"medical-bot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1177766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• shampoo often and wear hair off face\n",
      "• eat a well-balanced diet, avoiding foods that trigger\n",
      "flare-ups\n",
      "• unless told otherwise, give dry pimples a limited\n",
      "amount of sun exposur\n",
      "• do not pick or squeeze blemishes\n",
      "• reduce stress\n",
      "Resources\n",
      "BOOKS\n",
      "Balch, James F., and Phyllis A. Balch. “The Disorders: ...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4. Test\n",
    "# -------------------------------\n",
    "query = \"how to cure Acne?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(docs[0].page_content[:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba07a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
